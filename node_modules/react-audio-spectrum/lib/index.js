"use strict";

exports.__esModule = true;
exports["default"] = ReactAudioSpectrum;
var _react = _interopRequireWildcard(require("react"));
var React = _react;
var _utils = require("./utils");
function _getRequireWildcardCache(e) { if ("function" != typeof WeakMap) return null; var r = new WeakMap(), t = new WeakMap(); return (_getRequireWildcardCache = function _getRequireWildcardCache(e) { return e ? t : r; })(e); }
function _interopRequireWildcard(e, r) { if (!r && e && e.__esModule) return e; if (null === e || "object" != typeof e && "function" != typeof e) return { "default": e }; var t = _getRequireWildcardCache(r); if (t && t.has(e)) return t.get(e); var n = { __proto__: null }, a = Object.defineProperty && Object.getOwnPropertyDescriptor; for (var u in e) if ("default" !== u && {}.hasOwnProperty.call(e, u)) { var i = a ? Object.getOwnPropertyDescriptor(e, u) : null; i && (i.get || i.set) ? Object.defineProperty(n, u, i) : n[u] = e[u]; } return n["default"] = e, t && t.set(e, n), n; }
var PLAY_STATUS = /*#__PURE__*/function (PLAY_STATUS) {
  PLAY_STATUS["PAUSED"] = "PAUSED";
  PLAY_STATUS["PLAYING"] = "PLAYING";
  return PLAY_STATUS;
}(PLAY_STATUS || {});
function ReactAudioSpectrum(props) {
  var _props$id = props.id,
    id = _props$id === void 0 ? (0, _utils.getRandomId)(50) : _props$id,
    audioId = props.audioId,
    audioEle = props.audioEle,
    _props$width = props.width,
    width = _props$width === void 0 ? 300 : _props$width,
    _props$height = props.height,
    height = _props$height === void 0 ? 200 : _props$height,
    _props$capColor = props.capColor,
    capColor = _props$capColor === void 0 ? '#FFF' : _props$capColor,
    _props$capHeight = props.capHeight,
    capHeight = _props$capHeight === void 0 ? 2 : _props$capHeight,
    _props$meterColor = props.meterColor,
    meterColor = _props$meterColor === void 0 ? [{
      stop: 0,
      color: '#f00'
    }, {
      stop: 0.5,
      color: '#0CD7FD'
    }, {
      stop: 1,
      color: 'red'
    }] : _props$meterColor,
    _props$gap = props.gap,
    gap = _props$gap === void 0 ? 10 : _props$gap,
    _props$meterCount = props.meterCount,
    meterCount = _props$meterCount === void 0 ? 160 : _props$meterCount,
    _props$meterWidth = props.meterWidth,
    meterWidth = _props$meterWidth === void 0 ? 2 : _props$meterWidth,
    _props$silent = props.silent,
    silent = _props$silent === void 0 ? false : _props$silent;
  var _audioEleRef = (0, _react.useRef)();
  var _audioCanvasRef = (0, _react.useRef)();
  var _playStatusRef = (0, _react.useRef)();
  var _audioContextRef = (0, _react.useRef)();
  var _audioAnalyserRef = (0, _react.useRef)();
  var _mediaEleSourceRef = (0, _react.useRef)();
  var _animationIdRef = (0, _react.useRef)();
  var prepareElements = function prepareElements() {
    if (!audioId && !audioEle) {
      console.error('target audio not found!');
      return;
    } else if (audioId) {
      _audioEleRef.current = document.getElementById(audioId);
    } else {
      _audioEleRef.current = audioEle;
    }
    _audioCanvasRef.current = document.getElementById(id);
  };
  var handleBrowserVendors = function handleBrowserVendors() {
    // fix browser vender for AudioContext and requestAnimationFrame
    window.AudioContext = window.AudioContext || window.webkitAudioContext || window.mozAudioContext || window.msAudioContext;
    window.requestAnimationFrame = window.requestAnimationFrame || window.webkitRequestAnimationFrame || window.mozRequestAnimationFrame || window.msRequestAnimationFrame;
    window.cancelAnimationFrame = window.cancelAnimationFrame || window.webkitCancelAnimationFrame || window.mozCancelAnimationFrame || window.msCancelAnimationFrame;
  };
  var createAndConnect = function createAndConnect(audioEle) {
    // set audioContext
    try {
      _audioContextRef.current = new window.AudioContext();
    } catch (e) {
      console.error('Your browser does not support AudioContext', e);
    }
    // create and config Analyser
    if (!_audioAnalyserRef.current) {
      _audioAnalyserRef.current = _audioContextRef.current.createAnalyser();
      _audioAnalyserRef.current.smoothingTimeConstant = 0.8;
      _audioAnalyserRef.current.fftSize = 2048;
    }
    // create MediaElementSource and connect with both analyser and audioContext
    if (!_mediaEleSourceRef.current) {
      // @ts-ignore
      _mediaEleSourceRef.current = _audioContextRef.current.createMediaElementSource(audioEle);
      _mediaEleSourceRef.current.connect(_audioAnalyserRef.current);
      if (!silent) {
        _mediaEleSourceRef.current.connect(_audioContextRef.current.destination);
      }
    }
  };
  var drawSpectrum = function drawSpectrum() {
    var cwidth = width;
    var cheight = height - capHeight;
    var capYPositionArray = []; // store the vertical position of hte caps for the preivous frame
    var ctx = _audioCanvasRef.current.getContext('2d');
    var gradient = null;
    gradient = ctx.createLinearGradient(0, 0, 0, cheight);
    if (meterColor.constructor === Array) {
      var stops = meterColor;
      var len = stops.length;
      for (var i = 0; i < len; i++) {
        gradient.addColorStop(stops[i]['stop'], stops[i]['color']);
      }
    } else if (typeof meterColor === 'string') {
      gradient = meterColor;
    }
    var drawMeter = function drawMeter() {
      var array = new Uint8Array(_audioAnalyserRef.current.frequencyBinCount); // item value of array: 0 - 255
      _audioAnalyserRef.current.getByteFrequencyData(array);
      if (_playStatusRef.current === PLAY_STATUS.PAUSED) {
        for (var _i = array.length - 1; _i >= 0; _i--) {
          array[_i] = 0;
        }
        var allCapsReachBottom = !capYPositionArray.some(function (cap) {
          return cap > 0;
        });
        if (allCapsReachBottom) {
          ctx.clearRect(0, 0, cwidth, cheight + capHeight);
          cancelAnimationFrame(_animationIdRef.current); // since the sound is top and animation finished, stop the requestAnimation to prevent potential memory leak,THIS IS VERY IMPORTANT!
          return;
        }
      }
      var step = Math.round(array.length / meterCount); // sample limited data from the total array
      ctx.clearRect(0, 0, cwidth, cheight + capHeight);
      for (var _i2 = 0; _i2 < meterCount; _i2++) {
        var value = array[_i2 * step]; // pick one value out of every 2(step)
        if (capYPositionArray.length < Math.round(meterCount)) {
          capYPositionArray.push(value);
        }
        ;
        ctx.fillStyle = capColor;
        // draw the cap, with transition effect
        if (value < capYPositionArray[_i2]) {
          // let y = cheight - (--capYPositionArray[i])
          var preValue = --capYPositionArray[_i2];
          var _y = (256 - preValue) * cheight / 256;
          ctx.fillRect(_i2 * (meterWidth + gap), _y, meterWidth, capHeight);
        } else {
          // let y = cheight - value
          var _y2 = (256 - value) * cheight / 256;
          ctx.fillRect(_i2 * (meterWidth + gap), _y2, meterWidth, capHeight);
          capYPositionArray[_i2] = value;
        }
        ;
        ctx.fillStyle = gradient; // set the filllStyle to gradient for a better look

        // let y = cheight - value + capHeight
        var y = (256 - value) * cheight / 256 + capHeight;
        ctx.fillRect(_i2 * (meterWidth + gap), y, meterWidth, cheight); // the meter
      }
      _animationIdRef.current = requestAnimationFrame(drawMeter);
    };
    _animationIdRef.current = requestAnimationFrame(drawMeter);
  };
  var initAudioEvents = function initAudioEvents() {
    if (_audioEleRef.current) {
      _audioEleRef.current.onpause = function (e) {
        _playStatusRef.current = PLAY_STATUS.PAUSED;
      };
      _audioEleRef.current.onplay = function (e) {
        _playStatusRef.current = PLAY_STATUS.PLAYING;
        createAndConnect(_audioEleRef.current);
        drawSpectrum();
      };
    }
  };
  (0, _react.useEffect)(function () {
    prepareElements();
    handleBrowserVendors();
    initAudioEvents();
  }, []);
  return /*#__PURE__*/React.createElement("canvas", {
    id: id,
    width: width,
    height: height
  });
}